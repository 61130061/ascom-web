"use strict";(self.webpackChunkascom_web=self.webpackChunkascom_web||[]).push([[747],{3905:(e,t,a)=>{a.d(t,{Zo:()=>l,kt:()=>m});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var c=n.createContext({}),p=function(e){var t=n.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},l=function(e){var t=p(e.components);return n.createElement(c.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,c=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),d=p(a),h=r,m=d["".concat(c,".").concat(h)]||d[h]||u[h]||i;return a?n.createElement(m,o(o({ref:t},l),{},{components:a})):n.createElement(m,o({ref:t},l))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=h;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[d]="string"==typeof e?e:r,o[1]=s;for(var p=2;p<i;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}h.displayName="MDXCreateElement"},5e3:(e,t,a)=>{a.d(t,{Z:()=>i});var n=a(7294),r=a(2263);function i(e){let{src:t,style:a}=e;const{siteConfig:i}=(0,r.Z)();return n.createElement("img",{src:i.baseUrl+t,style:a})}},1289:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>c,toc:()=>l});var n=a(7462),r=(a(7294),a(3905)),i=a(5e3);const o={sidebar_position:2},s="Space",c={unversionedId:"space",id:"space",title:"Space",description:"Docking Operations",source:"@site/docs/space.mdx",sourceDirName:".",slug:"/space",permalink:"/ascom-web/docs/space",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/space.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Publication",permalink:"/ascom-web/docs/publication"},next:{title:"UAV",permalink:"/ascom-web/docs/uav"}},p={},l=[{value:"Docking Operations",id:"docking-operations",level:2},{value:"Research concept",id:"research-concept",level:3},{value:"Sponsors",id:"sponsors",level:3},{value:"Publications",id:"publications",level:3},{value:"Research on media",id:"research-on-media",level:3},{value:"Research status",id:"research-status",level:3}],d={toc:l},u="wrapper";function h(e){let{components:t,...o}=e;return(0,r.kt)(u,(0,n.Z)({},d,o,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"space"},"Space"),(0,r.kt)("h2",{id:"docking-operations"},"Docking Operations"),(0,r.kt)("h3",{id:"research-concept"},"Research concept"),(0,r.kt)("p",null,"The capture of a target spacecraft by a chaser is an on-orbit docking operation that requires an accurate, reliable, and robust object recognition algorithm. Vision-based guided spacecraft relative motion during close-proximity maneuvers has been consecutively applied using dynamic modeling as a spacecraft on-orbit service system. This research constructs a vision-based pose estimation model that performs image processing via a deep convolutional neural network. The pose estimation model was constructed by repurposing a modified pretrained GoogLeNet model with the available Unreal Engine 4 rendered dataset of the Soyuz spacecraft.\nIn the implementation, the convolutional neural network learns from the data samples to create correlations between the images and the spacecraft\u2019s six degrees-of-freedom parameters. The experiment has compared an exponential-based loss function and a weighted Euclidean-based loss function. Using the weighted Euclidean-based loss function, the implemented pose estimation model achieved moderately high performance with a position accuracy of 92.53 percent and an error of 1.2 m. The in-attitude prediction accuracy can reach 87.93 percent, and the errors in the three Euler angles do not exceed 7.6 degrees. This research can contribute to spacecraft detection and tracking problems. Although the finished vision-based model is specific to the environment of synthetic dataset, the model could be trained further to address actual docking operations in the future."),(0,r.kt)("div",{style:{textAlign:"center"}},(0,r.kt)("p",null,(0,r.kt)("img",{alt:"space-research-image-1",src:a(2100).Z,width:"331",height:"166"}))),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"space-research-image-2",src:a(9123).Z,width:"1280",height:"358"})),(0,r.kt)("h3",{id:"sponsors"},"Sponsors"),(0,r.kt)("p",null,"Docking operation cooperate with TSC pathfinder (capacity building of the project) by major join of AstroLab from GISTDA and NARIT."),(0,r.kt)("div",{style:{textAlign:"center"}},(0,r.kt)(i.Z,{src:"/space/3.png",style:{width:"300px"},mdxType:"Img"}),(0,r.kt)(i.Z,{src:"/space/4.jpg",style:{width:"300px"},mdxType:"Img"})),(0,r.kt)("h3",{id:"publications"},"Publications"),(0,r.kt)("p",null,"(Q2) Phisannupawong, Thaweerath, Patcharin Kamsing, Peerapong Torteeka, Sittiporn Channumsin, Utane Sawangwit, Warunyu Hematulin, Tanatthep Jarawan, Thanaporn Somjit, Soemsak Yooyen, Daniel Delahaye, and Pisit Boonsrimuang. 2020. \u201cVision-Based Spacecraft Pose Estimation via a Deep Convolutional Neural Network for Noncooperative Docking Operations\u201d Aerospace 7, no. 9: 126. ",(0,r.kt)("a",{parentName:"p",href:"https://doi.org/10.3390/aerospace7090126"},"https://doi.org/10.3390/aerospace7090126")),(0,r.kt)("p",null,"T. Phisannupawong, P. Kamsing, P. Tortceka and S. Yooyen, \u201cVision-based attitude estimation for spacecraft docking operation through deep learning algorithm,\u201d 2020 22nd International Conference on Advanced Communication Technology (ICACT), 2020, pp. 280-284, doi: 10.23919/ICACT48636.2020.9061445."),(0,r.kt)("p",null,"Full scholarship offer from Asia-Pacific Space Cooperation Organization(APSCO) for Mr. Thaweerath Phisannupawong to study master degree in Space Technology Application, Micro-satellite Technology, Beihang University, Beijing, China."),(0,r.kt)("p",null,"Full scholarship offer from China Scholarship Council(CSC) for Mr. Thaweerath Phisannupawong to study master degree in Control Science and Engineering, Research Field: Navigation, Guidance, and Control, Tsinghua University, Beijing, China."),(0,r.kt)("div",{style:{textAlign:"center"}},(0,r.kt)(i.Z,{src:"/space/5.png",style:{width:"120px",marginRight:"10px"},mdxType:"Img"}),(0,r.kt)(i.Z,{src:"/space/6.jpg",style:{width:"120px",marginRight:"10px"},mdxType:"Img"}),(0,r.kt)(i.Z,{src:"/space/7.jpg",style:{width:"120px",marginRight:"10px"},mdxType:"Img"}),(0,r.kt)(i.Z,{src:"/space/8.png",style:{width:"120px",marginRight:"10px"},mdxType:"Img"}),(0,r.kt)(i.Z,{src:"/space/9.jpg",style:{width:"120px",marginRight:"10px"},mdxType:"Img"})),(0,r.kt)("h3",{id:"research-on-media"},"Research on media"),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(1).Z})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.bangkokbiznews.com/tech/883691"},"https://www.bangkokbiznews.com/tech/883691")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.beartai.com/brief/sci-news/440976"},"https://www.beartai.com/brief/sci-news/440976"))),(0,r.kt)("p",null,"Joint signing a cooperation agreement on research and development of space technology for Thai space consortium(TSC) on 5 April 2021 at The Sukosol Hotel. Our laboratory participate in capacity building section of TSC-P by support with docking research and IAAI conference. The conference is online during 28-30 June 2021, and has important keynote speaker in field of space technology from many countries namely China, USA, Nepal, France, and Belgium."),(0,r.kt)("div",{style:{textAlign:"center"}},(0,r.kt)(i.Z,{src:"/space/10.jpg",style:{width:"400px",marginRight:"10px"},mdxType:"Img"}),(0,r.kt)(i.Z,{src:"/space/11.jpg",style:{width:"400px",marginRight:"10px"},mdxType:"Img"})),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(9217).Z,width:"1024",height:"468"})),(0,r.kt)("h3",{id:"research-status"},"Research status"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"The research will extend for Space Situational Awareness(SSA) in path of Satellite Collision Avoidance with private space company to make it commercial in the near future(on process of Raised Funding).")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"The extend research: Topic: \u201cSpace object trajectory by using state estimation based on CW equation and deep learning.\u201d on process for apply to be a project of APSCO (expect to be 2025)"))))}h.isMDXComponent=!0},2100:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/1-e45041c91e2c780dfca59e64cd8a74a3.png"},9217:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/12-d11d510d493fdbf4dd00d5a3919b4795.jpg"},9123:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/2-8fe193e18e9d526076e2996c445e1e2c.gif"},1:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Media-80bdf830a19e9bcf30066dc5de74dc7c.mp4"}}]);